# Optimizing DeepSeek: Int4 Quantization & Local Inference Engine ğŸš€

This project provides tools for quantizing and testing large language models from the DeepSeek family, specifically focusing on the DeepSeek-R1-Distill-Qwen-1.5B model. The primary goal is to make these models more accessible by reducing their memory footprint through quantization to Int4 precision and ONNX format conversion.

## ğŸ”¥ What is Model Quantization & Distillation? ğŸ”¥

### ğŸ“Š Quantization Explained

**Quantization** reduces the precision of numerical values in neural networks:

```
FP32 (32-bit) â¡ï¸ INT8 (8-bit) â¡ï¸ INT4 (4-bit)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”
   â”‚111010101â”‚  â†’   â”‚10101â”‚  â†’    â”‚1010â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”˜
  More Precise       Smaller      Tiny!
  (~6GB model)      (~1.5GB model)  (~1.7GB model)
```

Quantization **dramatically reduces** model size and memory footprint while attempting to preserve model quality!

### ğŸ§ª Distillation Explained 

**Distillation** is where a smaller model (student) learns from a larger model (teacher):

```
       Teacher Model              Student Model
       (Large, Slow)            (Small, Fast)
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”
       â”‚           â”‚     â†’        â”‚     â”‚
       â”‚  7B-175B  â”‚  Knowledge   â”‚ 1-3Bâ”‚
       â”‚ Parametersâ”‚  Transfer    â”‚Paramsâ”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”˜
```

DeepSeek-R1-**Distill**-Qwen-1.5B is already a distilled model that learned from larger models!

## ğŸš€ Quick Start Guide

### 1. ğŸ› ï¸ Setup Environment

```powershell
# Clone the repository (if you haven't already)
git clone https://github.com/shyamsridhar123/Quantization
cd VibeCollection/quantization

# Create and activate a virtual environment (recommended)
python -m venv .venv
.\.venv\Scripts\activate  # On Windows

# Install dependencies
pip install -r requirements.txt
```

### 2. ğŸŒŸ Export and Quantize the Model

```powershell
# Export the model to ONNX with position_ids support (fixes common 'logits' errors)
python reexport_with_position_ids.py --model-id deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B --output-dir ./onnx_fixed --quantize

# Alternatively, use the Jupyter notebooks for more control:
jupyter notebook notebooks/quantize_int4.ipynb
```

### 3. ğŸ” Test the Quantized Model

```powershell
# Interactive testing with direct ONNX Runtime approach
python direct_interactive_test.py --model-path ./quantized_model/onnx_int4

# Or run the full test suite
.\run_inference_tests.ps1
```

### 4. ğŸ“Š Benchmark Performance

```powershell
# Run performance benchmarks
python benchmark_model.py --model-path ./quantized_model/onnx_int4/model_quantized.onnx --num-threads 4
```

### 5. ğŸ”§ Troubleshooting

If you encounter issues:

```powershell
# Run diagnostics
python diagnose_onnx_model.py --model-path ./quantized_model/onnx_int4/model_quantized.onnx

# Try direct ONNX inference if Optimum integration fails
python enhanced_onnx_inference.py --model-path ./quantized_model/onnx_int4/model_quantized.onnx
```

## ğŸŒˆ Why Quantize Models? ğŸŒˆ

Quantizing models provides these amazing benefits:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸš€ 8x Smaller File Size             â”‚
â”‚ ğŸ’¾ ~70-75% Less Memory Usage        â”‚
â”‚ âš¡ Faster Inference Speed            â”‚
â”‚ ğŸ–¥ï¸ Run on Consumer Hardware         â”‚
â”‚ ğŸ”‹ Lower Energy Consumption         â”‚
â”‚ ğŸ  Enable Edge & Local Deployment   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Complete Workflow

### Step 1: ğŸ§° Prepare Your Environment

1. **Install Required Dependencies**:
   ```powershell
   pip install -r requirements.txt
   ```

2. **Verify Environment Setup**:
   ```powershell
   python environment_check.py
   ```

### Step 2: ğŸ”„ Export and Quantize the Model

1. **Re-Export Model with Position IDs** (Recommended for Qwen2-based models):
   ```powershell
   python reexport_with_position_ids.py --model-id deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B --output-dir ./onnx_fixed
   ```

2. **Quantize the Model to Int4**:
   ```powershell
   # Either use the re-exported model with quantization flag:
   python reexport_with_position_ids.py --model-id deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B --output-dir ./onnx_fixed --quantize
   
   # Or run the dedicated quantization script:
   python run_quantization.py --model-id deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B --output-dir ./quantized_model/onnx_int4
   
   # For more control, use the Jupyter notebook:
   jupyter notebook notebooks/quantize_int4.ipynb
   ```

### Step 3: ğŸ§ª Test the Quantized Model

1. **Run Diagnostics**:
   ```powershell
   python diagnose_onnx_model.py --model-path ./quantized_model/onnx_int4/model_quantized.onnx
   ```

2. **Interactive Testing**:
   ```powershell
   # Recommended approach (handles common issues):
   python direct_interactive_test.py --model-path ./quantized_model/onnx_int4
   
   # Alternative approach using Optimum:
   python interactive_test.py --model-path ./quantized_model/onnx_int4
   ```

3. **Batch Testing with Multiple Prompts**:
   ```powershell
   .\run_inference_tests.ps1
   ```

### Step 4: ğŸ“ˆ Benchmark and Compare

1. **Performance Benchmarking**:
   ```powershell
   python benchmark_model.py --model-path ./quantized_model/onnx_int4/model_quantized.onnx --num-threads 4
   ```

2. **Compare with Original Model** (Optional):
   ```powershell
   python compare_models.py --quantized-path ./quantized_model/onnx_int4 --model-id deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
   ```

3. **Generate HTML Report**:
   ```powershell
   python generate_report.py --results-dir ./inference_results --model-path ./quantized_model/onnx_int4 --output-file ./model_report.html
   ```

### Step 5: ğŸ”„ Format Conversion (Optional)

1. **Convert to FP16 for Troubleshooting**:
   ```powershell
   python convert_to_fp16.py --input-model ./quantized_model/onnx_int4/model_quantized.onnx --output-model ./quantized_model/onnx_fp16/model_fp16.onnx
   ```

## ğŸ” Quantization Deep Dive

Quantization in this repository follows this process:

```
                 Original Model (FP32)
                         â¬‡ï¸
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Export to ONNX                           â”‚
â”‚    - Convert model architecture to ONNX     â”‚
â”‚    - Add position_ids input (required fix) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â¬‡ï¸
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Calibration                              â”‚
â”‚    - Analyze weight distributions           â”‚
â”‚    - Determine optimal scaling factors      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â¬‡ï¸
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. INT4 Quantization                        â”‚
â”‚    - Apply scaling to weights               â”‚
â”‚    - Convert FP32 values to INT4 values     â”‚
â”‚    - Store quantization parameters          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â¬‡ï¸
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Optimization                             â”‚
â”‚    - Apply ONNX Runtime optimizations       â”‚
â”‚    - Fuse operations where possible         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â¬‡ï¸
                 Quantized Model (INT4)
```

## ğŸ—ï¸ Project Structure

### ğŸ§© Key Components

- **ğŸ”§ Quantization Tools**: Scripts and notebooks for exporting and quantizing models
- **ğŸ§ª Testing Framework**: Tools for testing, benchmarking, and troubleshooting the quantized models
- **ğŸ§¹ Maintenance Utilities**: Scripts for project organization and cleanup

### ğŸ“„ Important Scripts

#### ğŸ”„ Quantization and Export
- `reexport_with_position_ids.py` - ğŸ› ï¸ Exports the model with position_ids (fixes 'logits' errors)
- `run_quantization.py` - ğŸ”¢ Dedicated quantization script
- `quantize_int4.ipynb` - ğŸ““ Jupyter notebook for Int4 quantization

#### ğŸ§ª Testing and Inference
- `direct_interactive_test.py` - ğŸ’¬ Interactive testing using direct ONNX Runtime (most reliable)
- `interactive_test.py` - ğŸ¤– Interactive testing using Optimum integration
- `diagnose_onnx_model.py` - ğŸ” Detailed diagnostic checks for model issues
- `enhanced_onnx_inference.py` - âš¡ Direct ONNX Runtime inference with diagnostics
- `benchmark_model.py` - ğŸ“Š Performance benchmarking across different input sizes

#### ğŸ”„ Execution Scripts
- `run_inference_tests.ps1` - ğŸ§ª Comprehensive testing suite
- `run_master_test.ps1` - ğŸš€ Runs all tests in sequence

#### ğŸ§¹ Maintenance
- `maintain.bat` - ğŸ› ï¸ Central maintenance dashboard with menu options
- `cleanup_simple.ps1` - ğŸ§¹ Removes temporary files and Python cache

## â“ Common Issues and Solutions

### 1. 'Logits' Error During Generation âŒ

**Symptom**: `KeyError: 'logits'` or similar errors during model inference.

**Solution**: The model was likely exported without position_ids input. Re-export the model with:
```powershell
python reexport_with_position_ids.py --model-id deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B --output-dir ./onnx_fixed
```

### 2. Model Loading Failures âŒ

**Symptom**: Model fails to load or initialize.

**Solution**: Try using different loading parameters:
```python
# For Qwen2-based models
session_options = ort.SessionOptions()
session_options.intra_op_num_threads = 4
session = ort.InferenceSession(model_path, session_options)
```

### 3. Performance Issues â±ï¸

**Symptom**: Slow inference or high memory usage.

**Solution**: Adjust thread count to match your CPU cores:
```powershell
python benchmark_model.py --model-path ./model_path --num-threads 4  # Adjust to your CPU
```

## âš™ï¸ Configuration Options

- **ğŸ§µ Thread Count**: Adjust `--num-threads` based on your CPU. Generally, setting it to the number of physical cores works best.
- **ğŸ”¢ Token Limit**: Use `--max-tokens` to control generation length.
- **ğŸ“ Model Path**: All scripts accept `--model-path` to specify the model location.

## ğŸ“š The Technical Significance of This Project

This project demonstrates several important techniques:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸš€ ONNX Conversion                              â”‚
â”‚   - Hardware-agnostic deployment                â”‚
â”‚   - Runtime optimizations for any hardware      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ§® INT4 Quantization                            â”‚
â”‚   - Aggressive compression (8x smaller)         â”‚
â”‚   - 1.7GB model size (vs ~6GB for FP32)        â”‚
â”‚   - 1.5B parameters with vocabulary of 151,936  â”‚
â”‚   - Carefully preserves model quality           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ”§ Architecture-Specific Fixes                  â”‚
â”‚   - Fixes for Qwen2 architecture               â”‚
â”‚   - Custom position_ids handling               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ“Š Comprehensive Benchmarking                   â”‚
â”‚   - Performance across different inputs         â”‚
â”‚   - 2-4 tokens/sec generation speed            â”‚
â”‚   - 147-320ms first token latency              â”‚
â”‚   - Quality comparison with original            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”— Resources

- [ONNX Runtime Documentation](https://onnxruntime.ai/) ğŸ“š
- [Optimum Documentation](https://huggingface.co/docs/optimum/index) ğŸ¤—
- [DeepSeek-R1 Model Card](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B) ğŸ§ 

## ğŸ“œ License

This project is licensed under the MIT No Attribution License - see the [LICENSE](LICENSE) file for details.

## ğŸ‘¥ Contributing

Contributions are welcome! See `CONTRIBUTING.md` for guidelines. ğŸŒŸ

## âœ¨ Why This Matters

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚  ğŸ–¥ï¸  Run Advanced AI locally on your own hardware               â”‚
â”‚  ğŸ”’  Keep your data private - no cloud required                 â”‚
â”‚  ğŸ’¸  No subscription costs or API fees                          â”‚
â”‚  ğŸŒ±  Lower environmental impact than cloud inference            â”‚
â”‚  ğŸ› ï¸  Full control over inference parameters                     â”‚
â”‚  ğŸš€  Deploy in resource-constrained environments                â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Complete Workflow

### Step 1: Prepare Your Environment

1. **Install Required Dependencies**:
   ```powershell
   pip install -r requirements.txt
   ```

2. **Verify Environment Setup**:
   ```powershell
   python environment_check.py
   ```

### Step 2: Export and Quantize the Model

1. **Re-Export Model with Position IDs** (Recommended for Qwen2-based models):
   ```powershell
   python reexport_with_position_ids.py --model-id deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B --output-dir ./onnx_fixed
   ```

2. **Quantize the Model to Int4**:
   ```powershell
   # Either use the re-exported model with quantization flag:
   python reexport_with_position_ids.py --model-id deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B --output-dir ./onnx_fixed --quantize
   
   # Or run the dedicated quantization script:
   python run_quantization.py --model-id deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B --output-dir ./quantized_model/onnx_int4
   
   # For more control, use the Jupyter notebook:
   jupyter notebook notebooks/quantize_int4.ipynb
   ```

### Step 3: Test the Quantized Model

1. **Run Diagnostics**:
   ```powershell
   python diagnose_onnx_model.py --model-path ./quantized_model/onnx_int4/model_quantized.onnx
   ```

2. **Interactive Testing**:
   ```powershell
   # Recommended approach (handles common issues):
   python direct_interactive_test.py --model-path ./quantized_model/onnx_int4
   
   # Alternative approach using Optimum:
   python interactive_test.py --model-path ./quantized_model/onnx_int4
   ```

3. **Batch Testing with Multiple Prompts**:
   ```powershell
   .\run_inference_tests.ps1
   ```

### Step 4: Benchmark and Compare

1. **Performance Benchmarking**:
   ```powershell
   python benchmark_model.py --model-path ./quantized_model/onnx_int4/model_quantized.onnx --num-threads 4
   ```

2. **Compare with Original Model** (Optional):
   ```powershell
   python compare_models.py --quantized-path ./quantized_model/onnx_int4 --model-id deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
   ```

3. **Generate HTML Report**:
   ```powershell
   python generate_report.py --results-dir ./inference_results --model-path ./quantized_model/onnx_int4 --output-file ./model_report.html
   ```

### Step 5: Format Conversion (Optional)

1. **Convert to FP16 for Troubleshooting**:
   ```powershell
   python convert_to_fp16.py --input-model ./quantized_model/onnx_int4/model_quantized.onnx --output-model ./quantized_model/onnx_fp16/model_fp16.onnx
   ```

## Project Structure

### Key Components

- **Quantization Tools**: Scripts and notebooks for exporting and quantizing models
- **Testing Framework**: Tools for testing, benchmarking, and troubleshooting the quantized models
- **Maintenance Utilities**: Scripts for project organization and cleanup

### Important Scripts

#### Quantization and Export
- `reexport_with_position_ids.py` - Exports the model with position_ids (fixes 'logits' errors)
- `run_quantization.py` - Dedicated quantization script
- `quantize_int4.ipynb` - Jupyter notebook for Int4 quantization

#### Testing and Inference
- `direct_interactive_test.py` - Interactive testing using direct ONNX Runtime (most reliable)
- `interactive_test.py` - Interactive testing using Optimum integration
- `diagnose_onnx_model.py` - Detailed diagnostic checks for model issues
- `enhanced_onnx_inference.py` - Direct ONNX Runtime inference with diagnostics
- `benchmark_model.py` - Performance benchmarking across different input sizes

#### Execution Scripts
- `run_inference_tests.ps1` - Comprehensive testing suite
- `run_master_test.ps1` - Runs all tests in sequence

#### Maintenance
- `maintain.bat` - Central maintenance dashboard with menu options
- `cleanup_simple.ps1` - Removes temporary files and Python cache

## Common Issues and Solutions

### 1. 'Logits' Error During Generation

**Symptom**: `KeyError: 'logits'` or similar errors during model inference.

**Solution**: The model was likely exported without position_ids input. Re-export the model with:
```powershell
python reexport_with_position_ids.py --model-id deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B --output-dir ./onnx_fixed
```

### 2. Model Loading Failures

**Symptom**: Model fails to load or initialize.

**Solution**: Try using different loading parameters:
```python
# For Qwen2-based models
session_options = ort.SessionOptions()
session_options.intra_op_num_threads = 4
session = ort.InferenceSession(model_path, session_options)
```

### 3. Performance Issues

**Symptom**: Slow inference or high memory usage.

**Solution**: Adjust thread count to match your CPU cores:
```powershell
python benchmark_model.py --model-path ./model_path --num-threads 4  # Adjust to your CPU
```

## Configuration Options

- **Thread Count**: Adjust `--num-threads` based on your CPU. Generally, setting it to the number of physical cores works best.
- **Token Limit**: Use `--max-tokens` to control generation length.
- **Model Path**: All scripts accept `--model-path` to specify the model location.

## Resources

- [ONNX Runtime Documentation](https://onnxruntime.ai/)
- [Optimum Documentation](https://huggingface.co/docs/optimum/index)
- [DeepSeek-R1 Model Card](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)

## License

This project is licensed under the MIT No Attribution License - see the [LICENSE](LICENSE) file for details.

## Contributing

Contributions are welcome! See `CONTRIBUTING.md` for guidelines.

## Why This Matters

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚  ğŸ–¥ï¸  Run Advanced AI locally on your own hardware               â”‚
â”‚  ğŸ”’  Keep your data private - no cloud required                 â”‚
â”‚  ğŸ’¸  No subscription costs or API fees                          â”‚
â”‚  ğŸŒ±  Lower environmental impact than cloud inference            â”‚
â”‚  ğŸ› ï¸  Full control over inference parameters                     â”‚
â”‚  ğŸš€  Deploy in resource-constrained environments                â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```