{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantizing DeepSeek-R1-Distill-Qwen-1.5B\n",
    "\n",
    "This notebook demonstrates how to load the `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B` model with quantization options for CUDA (using `bitsandbytes` for 4-bit or 8-bit) or load it in default precision on CPU.\n",
    "\n",
    "**Note:** Please verify the exact model ID on Hugging Face Hub. `bitsandbytes` 4-bit/8-bit quantization is primarily for NVIDIA GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Install Libraries\n",
    "\n",
    "Uncomment and run the following cell if you haven't installed the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 # Adjust cuXXX to your CUDA version\n",
    "# !pip install transformers accelerate bitsandbytes sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set your model ID and choose your device and quantization options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"  # !!! VERIFY THIS MODEL ID !!!\n",
    "\n",
    "# --- Choose your configuration ---\n",
    "USE_CUDA = True  # Set to True for GPU (with quantization), False for CPU (default precision)\n",
    "QUANTIZATION_BITS = 4  # Choose 4 or 8. Only applicable if USE_CUDA is True.\n",
    "# ---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import Libraries and Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Current GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "    print(f\"Tokenizer for {MODEL_ID} loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer for {MODEL_ID}: {e}\")\n",
    "    print(\"Please ensure the MODEL_ID is correct and you have an internet connection.\")\n",
    "    tokenizer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model\n",
    "\n",
    "This section loads the model based on the configuration set above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "quantization_config = None\n",
    "\n",
    "if tokenizer: # Proceed only if tokenizer was loaded\n",
    "    if USE_CUDA and torch.cuda.is_available():\n",
    "        print(f\"Attempting to load model on CUDA with {QUANTIZATION_BITS}-bit quantization...\")\n",
    "        if QUANTIZATION_BITS == 4:\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",        # nf4 is a common choice for 4-bit\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16 # Or torch.float16 if bfloat16 is not supported\n",
    "            )\n",
    "            print(\"Using 4-bit quantization (NF4) with bfloat16 compute dtype.\")\n",
    "        elif QUANTIZATION_BITS == 8:\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_8bit=True\n",
    "            )\n",
    "            print(\"Using 8-bit quantization.\")\n",
    "        else:\n",
    "            print(f\"Unsupported QUANTIZATION_BITS: {QUANTIZATION_BITS}. Loading in default precision on CUDA.\")\n",
    "            # Fallback to default precision on CUDA if bits not 4 or 8\n",
    "            quantization_config = None \n",
    "\n",
    "        try:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                MODEL_ID,\n",
    "                quantization_config=quantization_config if quantization_config else None, # Pass config only if defined\n",
    "                device_map=\"auto\", # Automatically distributes model layers\n",
    "                trust_remote_code=True # Add if model requires it\n",
    "            )\n",
    "            print(f\"Model {MODEL_ID} loaded successfully on CUDA.\")\n",
    "            if quantization_config:\n",
    "                 print(f\"Model memory footprint: {model.get_memory_footprint() / (1024**3):.2f} GB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model on CUDA: {e}\")\n",
    "            print(\"Ensure 'bitsandbytes' is correctly installed and your GPU is compatible.\")\n",
    "\n",
    "    elif not USE_CUDA:\n",
    "        print(f\"Attempting to load model {MODEL_ID} on CPU in default precision...\")\n",
    "        print(\"(Note: bitsandbytes 4/8-bit quantization is primarily for CUDA GPUs)\")\n",
    "        try:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                MODEL_ID,\n",
    "                device_map=\"cpu\",\n",
    "                trust_remote_code=True # Add if model requires it\n",
    "            )\n",
    "            print(f\"Model {MODEL_ID} loaded successfully on CPU.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model on CPU: {e}\")\n",
    "    else:\n",
    "        print(\"CUDA selected but not available. Please check your PyTorch and CUDA setup.\")\n",
    "else:\n",
    "    print(\"Tokenizer not loaded. Skipping model loading.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Inference\n",
    "\n",
    "Let's try a simple prompt to see if the model generates text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model and tokenizer:\n",
    "    prompt = \"What is the capital of France?\"\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "\n",
    "    # Determine the device of the input tensors\n",
    "    # For models loaded with device_map='auto', the inputs should ideally be on the same device \n",
    "    # as the first parameter of the model, or let `generate` handle it.\n",
    "    # If model is explicitly on CPU, inputs should be on CPU.\n",
    "    # If model is on CUDA (potentially sharded), `generate` usually handles input placement well.\n",
    "    \n",
    "# A common approach for device_map='auto' or single GPU:\n",
    "# device = next(model.parameters()).device \n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Simpler approach, often works as `generate` can handle it:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # If you explicitly moved the entire model to one device (e.g., model.to('cuda:0') or model.to('cpu'))\n",
    "    # then you should move inputs to that device:\n",
    "    # inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # For models loaded with device_map=\"auto\", especially sharded ones,\n",
    "    # it's often best to let the `generate` method handle the device placement of inputs internally.\n",
    "    # If you encounter device mismatch errors, you might need to explicitly move `input_ids`\n",
    "    # to `next(model.parameters()).device`.\n",
    "\n",
    "    print(\"Generating response...\")\n",
    "    try:\n",
    "        # Ensure inputs are on the correct device if not handled automatically\n",
    "        # This is a robust way if model is sharded via device_map=\"auto\"\n",
    "        if USE_CUDA and torch.cuda.is_available() and hasattr(model, 'hf_device_map'):\n",
    "            # For sharded models, let `generate` handle the device placement.\n",
    "            # If you run into issues, you can explicitly move inputs to the first device:\n",
    "            # first_device = list(model.hf_device_map.values())[0]\n",
    "            # inputs = {k: v.to(first_device) for k, v in inputs.items()}\n",
    "            pass  # Rely on generate's internal handling for device_map='auto'\n",
    "        elif not USE_CUDA:\n",
    "             inputs = {k: v.to('cpu') for k, v in inputs.items()}\n",
    "\n",
    "        outputs = model.generate(**inputs, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id)\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        print(\"\\nGenerated Text:\")\n",
    "        print(generated_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference: {e}\")\n",
    "        if \"expected scalar type Half but found Float\" in str(e).lower() and USE_CUDA and QUANTIZATION_BITS == 4:\n",
    "            print(\"Hint: This error might occur if bnb_4bit_compute_dtype=torch.float16 was used and the GPU has limited float16 support, or an operation wasn't correctly cast. Try torch.bfloat16 if your GPU supports it (Ampere architecture or newer), or remove bnb_4bit_compute_dtype.\")\n",
    "        elif \"CUDA out of memory\" in str(e):\n",
    "            print(\"Hint: CUDA out of memory. Try a smaller model, reduce batch size (if applicable for training/fine-tuning), or use more aggressive quantization if possible.\")\n",
    "else:\n",
    "    print(\"Model or tokenizer not loaded. Skipping inference test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "This notebook provided a basic framework for loading the DeepSeek model with quantization options.\n",
    "\n",
    "**Next Steps:**\n",
    "*   **Verify Model ID:** Double-check the `MODEL_ID` on Hugging Face Hub.\n",
    "*   **Evaluate Performance:** Measure actual inference speed and output quality.\n",
    "*   **Advanced Quantization:** For potentially better results (especially if 4-bit with `bitsandbytes` direct load isn't sufficient), explore methods like GPTQ (`auto-gptq`) or AWQ (`autoawq`), which require a calibration dataset.\n",
    "*   **CPU Quantization:** If you need optimized CPU performance, look into `torch.quantization` (dynamic or static quantization) or using `optimum` with ONNX Runtime and its quantization capabilities for CPU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
