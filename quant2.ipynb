{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning DeepSeek-R1-Distill-Qwen-1.5B with QLoRA\n",
    "\n",
    "This notebook provides a step-by-step guide to fine-tuning the `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B` model using QLoRA (Quantized Low-Rank Adaptation).\n",
    "\n",
    "**QLoRA** is a Parameter-Efficient Fine-Tuning (PEFT) technique that significantly reduces memory usage by:\n",
    "1. Quantizing the pre-trained model to 4-bit.\n",
    "2. Attaching small, trainable Low-Rank Adapters (LoRA) to the model.\n",
    "3. Training only these adapters while keeping the base model frozen.\n",
    "\n",
    "This allows fine-tuning large models on relatively modest hardware.\n",
    "\n",
    "**Prerequisites:**\n",
    "- An NVIDIA GPU with CUDA installed (>=12GB VRAM recommended).\n",
    "- Python 3.8+.\n",
    "- Necessary libraries (will be installed below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Install Libraries\n",
    "\n",
    "Uncomment and run the following cell to install the required Python packages. Restart the kernel after installation if prompted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 # Adjust cuXXX to your CUDA version\n",
    "# !pip install transformers==4.38.2 # Pinning version for stability with TRL and PEFT at the time of writing\n",
    "# !pip install datasets peft accelerate bitsandbytes trl sentencepiece scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Define the model ID, dataset parameters, and training configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# --- Model Configuration ---\n",
    "BASE_MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"  # !!! VERIFY THIS MODEL ID !!!\n",
    "FINE_TUNED_MODEL_NAME = \"deepseek-r1-distill-qwen-1.5b-finetuned-qlora\" # Name for saving the adapter\n",
    "\n",
    "# --- Dataset Configuration (Dummy Example) ---\n",
    "# For this example, we'll create a tiny dummy dataset.\n",
    "# In a real scenario, you would load your custom dataset here.\n",
    "# The dataset should ideally be in a format like: {'text': [\"instruction: ... output: ...\", ...]}\n",
    "# or structured for SFTTrainer (e.g., instruction, output columns).\n",
    "DUMMY_DATASET_PATH = \"dummy_instruction_dataset.jsonl\"\n",
    "\n",
    "# --- QLoRA Configuration ---\n",
    "LORA_R = 16                     # LoRA rank (dimension of the low-rank matrices)\n",
    "LORA_ALPHA = 32                 # LoRA alpha (scaling factor)\n",
    "LORA_DROPOUT = 0.05             # Dropout probability for LoRA layers\n",
    "# Modules to target with LoRA. These are often attention projection layers.\n",
    "# You might need to inspect the model architecture to find appropriate module names.\n",
    "# Common names for Qwen-like models: \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "LORA_TARGET_MODULES = [\n",
    "    \"q_proj\",\n",
    "    \"v_proj\",\n",
    "    # \"k_proj\", # Sometimes k_proj is not targeted or can cause issues, experiment as needed\n",
    "    # \"o_proj\",\n",
    "    # \"gate_proj\",\n",
    "    # \"up_proj\",\n",
    "    # \"down_proj\"\n",
    "]\n",
    "\n",
    "# --- Training Arguments ---\n",
    "OUTPUT_DIR = \"./results_qlora\"\n",
    "BATCH_SIZE = 1                  # Adjust based on your VRAM (1 is safest for large models)\n",
    "MICRO_BATCH_SIZE = 1            # Per device batch size\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "LEARNING_RATE = 2e-4\n",
    "NUM_TRAIN_EPOCHS = 1            # Start with 1 epoch for testing\n",
    "MAX_SEQ_LENGTH = 512            # Maximum sequence length for tokenization\n",
    "LOGGING_STEPS = 10\n",
    "SAVE_STEPS = 50                 # Save checkpoints every N steps\n",
    "\n",
    "# --- Device Configuration ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if DEVICE == \"cpu\":\n",
    "    print(\"WARNING: QLoRA is designed for GPUs. Training on CPU will be extremely slow and may not work as intended.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Dataset\n",
    "\n",
    "For fine-tuning, you need a dataset. We'll create a small dummy dataset in JSONL format for demonstration. Each line will contain an instruction and its expected output.\n",
    "\n",
    "A common format for instruction fine-tuning is a text column where each entry is a string like:\n",
    "```\n",
    "### Instruction:\n",
    "{Your instruction here}\n",
    "\n",
    "### Response:\n",
    "{Your desired response here}\n",
    "```\n",
    "The `SFTTrainer` from `trl` can work well with this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Create a dummy dataset file\n",
    "dummy_data = [\n",
    "    {\"instruction\": \"What is the capital of France?\", \"output\": \"The capital of France is Paris.\"},\n",
    "    {\"instruction\": \"Explain the theory of relativity in simple terms.\", \"output\": \"Einstein's theory of relativity has two parts. Special relativity says that the laws of physics are the same for all non-accelerating observers, and that the speed of light in a vacuum is the same for all observers, regardless of the motion of the light source. General relativity explains gravity as a curvature of spacetime caused by mass and energy.\"},\n",
    "    {\"instruction\": \"Write a short poem about a cat.\", \"output\": \"A furry friend, with eyes so bright,\\nChasing shadows in the fading light.\\nA gentle purr, a soft, warm paw,\\nThe finest creature, by nature's law.\"},\n",
    "    {\"instruction\": \"Translate 'Hello, world!' to Spanish.\", \"output\": \"Â¡Hola, mundo!\"}\n",
    "]\n",
    "\n",
    "with open(DUMMY_DATASET_PATH, 'w') as f:\n",
    "    for item in dummy_data:\n",
    "        # We will format it into a single text field for SFTTrainer\n",
    "        # You can also have separate 'instruction' and 'output' columns and use a formatting_func\n",
    "        text_data = f\"### Instruction:\\n{item['instruction']}\\n\\n### Response:\\n{item['output']}\"\n",
    "        f.write(json.dumps({\"text\": text_data}) + \"\\n\")\n",
    "\n",
    "print(f\"Dummy dataset created at: {DUMMY_DATASET_PATH}\")\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"json\", data_files=DUMMY_DATASET_PATH, split=\"train\")\n",
    "\n",
    "print(\"\\nDataset loaded:\")\n",
    "print(dataset)\n",
    "print(\"\\nExample entry:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Base Model & Tokenizer for QLoRA\n",
    "\n",
    "We'll load the base model in 4-bit precision using `bitsandbytes` and configure its tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# Configure BitsAndBytes for 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",                # Use NF4 (NormalFloat4) data type for weights\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,    # Use bfloat16 for computation (if supported, else float16)\n",
    "    bnb_4bit_use_double_quant=True,           # Use a second quantization after the first one to save more memory\n",
    ")\n",
    "\n",
    "print(f\"Loading base model: {BASE_MODEL_ID} with QLoRA config...\")\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_ID,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\", # Automatically distribute model layers (recommended for QLoRA)\n",
    "        trust_remote_code=True # Necessary for some models\n",
    "    )\n",
    "    print(\"Base model loaded successfully.\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)\n",
    "    # Set padding token if not already set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\" # Important for Causal LM\n",
    "    print(\"Tokenizer loaded and configured.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model or tokenizer: {e}\")\n",
    "    print(\"Please ensure the MODEL_ID is correct, you have internet access, and your GPU/CUDA setup is compatible with bitsandbytes.\")\n",
    "    model = None\n",
    "    tokenizer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure LoRA\n",
    "\n",
    "Set up the LoRA configuration using `peft` and apply it to the loaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "if model:\n",
    "    # Prepare the model for k-bit training (important for QLoRA)\n",
    "    # This function does a few things to make the model compatible with k-bit training and PEFT:\n",
    "    # - Casts all non INT8 modules to full precision (fp32) for stability\n",
    "    # - Adds a forward hook to the model to enable gradient checkpointing if specified\n",
    "    # - Upcasts the layer norms in float32 for stability\n",
    "    if hasattr(model, 'hf_device_map'): # Check if model is loaded with device_map\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "    else: # If not using device_map, ensure model is on the correct device before this step\n",
    "        model.to(DEVICE)\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        target_modules=LORA_TARGET_MODULES,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        bias=\"none\",  # Typically set to 'none' for LoRA\n",
    "        task_type=\"CAUSAL_LM\" # Important for Causal Language Models\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    print(\"\\nLoRA configured and applied to the model.\")\n",
    "    model.print_trainable_parameters()\n",
    "else:\n",
    "    print(\"Model not loaded, skipping LoRA configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Set Training Arguments & Initialize Trainer\n",
    "\n",
    "We'll use `TrainingArguments` from `transformers` and `SFTTrainer` from the `trl` library. `SFTTrainer` is specifically designed for supervised fine-tuning of language models and simplifies the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "if model and tokenizer and dataset:\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "        max_grad_norm=0.3, # Gradient clipping\n",
    "        lr_scheduler_type=\"cosine\", # Learning rate scheduler\n",
    "        warmup_ratio=0.03, # Warmup steps for learning rate scheduler\n",
    "        logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "        logging_steps=LOGGING_STEPS,\n",
    "        save_steps=SAVE_STEPS,\n",
    "        save_total_limit=2, # Keep only the last 2 checkpoints\n",
    "        fp16=False, # Set to False when bnb_4bit_compute_dtype is bfloat16. If float16, set to True.\n",
    "        bf16=True if torch.cuda.is_bf16_supported() and bnb_config.bnb_4bit_compute_dtype == torch.bfloat16 else False, # Enable bfloat16 if supported and configured\n",
    "        optim=\"paged_adamw_32bit\", # Use paged AdamW optimizer for memory efficiency\n",
    "        # report_to=\"tensorboard\" # Optional: for logging to TensorBoard\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        dataset_text_field=\"text\",  # Name of the text field in your dataset\n",
    "        max_seq_length=MAX_SEQ_LENGTH, # Max sequence length for tokenization\n",
    "        packing=False, # Optional: packing multiple short examples into one sequence for efficiency\n",
    "        # formatting_func=formatting_prompts_func, # If your dataset needs custom formatting\n",
    "        peft_config=lora_config, # Pass LoRA config here if not already applied to model\n",
    "    )\n",
    "    print(\"\\nTrainer initialized.\")\n",
    "else:\n",
    "    print(\"Model, tokenizer, or dataset not available. Skipping trainer initialization.\")\n",
    "    trainer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Start Fine-tuning\n",
    "\n",
    "Now, we can start the fine-tuning process. This will take time depending on your dataset size, hardware, and training configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if trainer:\n",
    "    print(\"\\nStarting fine-tuning...\")\n",
    "    try:\n",
    "        trainer.train()\n",
    "        print(\"Fine-tuning completed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "else:\n",
    "    print(\"Trainer not initialized. Skipping training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save the Fine-tuned Model (Adapter)\n",
    "\n",
    "After training, save the LoRA adapter. This contains the fine-tuned weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if trainer:\n",
    "    print(f\"\\nSaving fine-tuned LoRA adapter to: {FINE_TUNED_MODEL_NAME}\")\n",
    "    try:\n",
    "        # trainer.save_model(FINE_TUNED_MODEL_NAME) # SFTTrainer saves the full model by default if not using PEFT directly\n",
    "        # When using PEFT model directly with SFTTrainer, the adapter is saved within the output_dir of TrainingArguments.\n",
    "        # To save only the adapter explicitly:\n",
    "        model.save_pretrained(FINE_TUNED_MODEL_NAME) \n",
    "        tokenizer.save_pretrained(FINE_TUNED_MODEL_NAME) # Also save the tokenizer\n",
    "        print(f\"Adapter and tokenizer saved to {FINE_TUNED_MODEL_NAME}\")\n",
    "        \n",
    "        # You can find the adapter in OUTPUT_DIR/checkpoint-XXXX/adapter_model.safetensors or similar\n",
    "        # or directly in FINE_TUNED_MODEL_NAME if saved explicitly.\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {e}\")\n",
    "else:\n",
    "    print(\"Trainer not initialized or training failed. Skipping model saving.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inference with the Fine-tuned Model\n",
    "\n",
    "To use the fine-tuned model, load the base model again (in 4-bit) and then apply the trained LoRA adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "import os\n",
    "\n",
    "if os.path.exists(FINE_TUNED_MODEL_NAME):\n",
    "    print(\"\\nLoading base model and fine-tuned adapter for inference...\")\n",
    "    \n",
    "    # Load the base model in 4-bit (as it was during training)\n",
    "    base_model_for_inference = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_ID,\n",
    "        quantization_config=bnb_config, # Same BitsAndBytesConfig as training\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # Load the LoRA adapter\n",
    "    # The FINE_TUNED_MODEL_NAME directory should contain 'adapter_model.safetensors' or 'adapter_model.bin'\n",
    "    # and 'adapter_config.json'\n",
    "    ft_model = PeftModel.from_pretrained(base_model_for_inference, FINE_TUNED_MODEL_NAME)\n",
    "    \n",
    "    # Optional: Merge LoRA weights with the base model for faster inference (requires more memory)\n",
    "    # print(\"Merging LoRA adapter with base model...\")\n",
    "    # ft_model = ft_model.merge_and_unload() # This creates a new model with merged weights\n",
    "    \n",
    "    ft_tokenizer = AutoTokenizer.from_pretrained(FINE_TUNED_MODEL_NAME) # Load tokenizer saved with adapter\n",
    "    if ft_tokenizer.pad_token is None:\n",
    "        ft_tokenizer.pad_token = ft_tokenizer.eos_token\n",
    "    ft_tokenizer.padding_side = \"right\"\n",
    "\n",
    "    print(\"Fine-tuned model ready for inference.\")\n",
    "\n",
    "    # --- Test Inference ---\n",
    "    prompt = \"### Instruction:\\nWhat is QLoRA?\\n\\n### Response:\\n\" # Use the same format as training data\n",
    "    print(f\"\\nTest Prompt: {prompt}\")\n",
    "\n",
    "    inputs = ft_tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_SEQ_LENGTH).to(DEVICE)\n",
    "    \n",
    "    # Ensure the model is in evaluation mode\n",
    "    ft_model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = ft_model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=100,\n",
    "            pad_token_id=ft_tokenizer.eos_token_id, # Important for generation\n",
    "            eos_token_id=ft_tokenizer.eos_token_id,\n",
    "            do_sample=True, # For more creative responses\n",
    "            top_p=0.9,\n",
    "            temperature=0.7\n",
    "        )\n",
    "    \n",
    "    response_text = ft_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"\\nGenerated Response:\\n{response_text}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Fine-tuned model adapter not found at {FINE_TUNED_MODEL_NAME}. Skipping inference test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion and Next Steps\n",
    "\n",
    "This notebook demonstrated the process of fine-tuning `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B` using QLoRA.\n",
    "\n",
    "**Key Takeaways:**\n",
    "- QLoRA enables fine-tuning large models on limited hardware by quantizing the base model and training small adapters.\n",
    "- The `peft` and `trl` libraries from Hugging Face significantly simplify this process.\n",
    "- Careful configuration of model loading, LoRA parameters, and training arguments is crucial.\n",
    "\n",
    "**Next Steps:**\n",
    "- **Use Your Custom Dataset:** Replace the dummy dataset with your actual data, ensuring it's properly formatted.\n",
    "- **Hyperparameter Tuning:** Experiment with `LORA_R`, `LORA_ALPHA`, `LEARNING_RATE`, `NUM_TRAIN_EPOCHS`, `BATCH_SIZE`, etc., to optimize performance for your specific task and dataset.\n",
    "- **Target Module Selection:** Investigate the model architecture to choose the most effective `LORA_TARGET_MODULES`.\n",
    "- **Evaluation:** Implement a robust evaluation strategy to measure the performance of your fine-tuned model on a held-out test set.\n",
    "- **Advanced Techniques:** Explore techniques like packing (in `SFTTrainer`) for efficiency, or different schedulers.\n",
    "- **Push to Hub:** Share your fine-tuned adapters on the Hugging Face Hub."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12" 
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}